import argparse
import csv
import pandas as pd
from pathlib import Path
from peft import LoraConfig, get_peft_model
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig, get_linear_schedule_with_warmup

### Methods and Classes ###

# Define the model which uses CodeBERT as a base with a single one-hot classification layer on top
# The [CLS] token holds all the information of the rest of the input for BERT models and sits as the first embedding
# The last token has seen all previous tokens in a decoder-only based model and sits as the last embedding
class VulnerabilityPrediction(nn.Module):
    def __init__(self, model_name, model, embedding_dim):
        super(VulnerabilityPrediction, self).__init__()
        self.model_name = model_name
        self.model = model
        self.dense = nn.Linear(embedding_dim, embedding_dim)
        self.dropout = nn.Dropout(0.5)
        self.classifier = nn.Linear(embedding_dim, 2)
    
    def forward(self, input_ids, attention_mask):
        x = self.model(input_ids=input_ids, attention_mask=attention_mask)
        if self.model_name == 'codebert':
            x = x.last_hidden_state[:, 0, :]
        elif self.model_name == 'llama':
            x = x.last_hidden_state[:, -1, :]
        x = x.to(torch.float32)
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.classifier(x)
        return x

# Dataset class will tokenize the input data
class VulnerabilityDataset(Dataset):
    def __init__(self, processed_func, target, tokenizer):
        self.func = processed_func
        self.labels = target
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        tokenized_func = self.tokenizer(self.func[idx], return_tensors='pt', truncation=True, padding='max_length', max_length=512)
        label = torch.tensor(self.labels[idx])
        return tokenized_func, label
    
# Metric logging
def log_metrics(model_name, acc, prec, recall, mcc, f1, tp, fp, tn, fn, test_loss, filename):
    row = [model_name, acc, prec, recall, mcc, f1, tp, fp, tn, fn, test_loss]
    try:
        with open(filename, mode='x', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['Model','Accuracy','Precision','Recall','MCC','F1','TP','FP','TN','FN', 'Test_loss'])
    except FileExistsError:
        pass
    with open(filename, mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(row)

### Script Start ###
if __name__ == '__main__':
    # Command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('model', type=str, choices=['codebert','llama'], help='Model selected: codebert or llama')
    # parser.add_argument('iterations', type=int, help='Amount of models that will be trained')
    args = parser.parse_args()

    base_model_name = args.model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # QLoRA configuration
    # LLama config
    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", 
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
    )


    # Choosing model and setting appropriate parameters
    if base_model_name == 'codebert':
        model_name = 'microsoft/codebert-base'
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        base_model = AutoModel.from_pretrained(model_name, quantization_config=bnb_config).to(device)
        embedding_dim = 768
        task_type='FEATURE_EXTRACTION'
        target_modules=['query','value']
        model_dir = 'best_model_codebert.pth'
    elif base_model_name == 'llama':
        model_name = 'meta-llama/Llama-3.2-1B'
        embedding_dim = 2048
        model_dir = 'best_model_llama.pth'
        # This is a gated model, please save your Hugging Face read-only token in token.txt
        with open('token.txt', 'r') as file:
            auth_token = file.read().strip()
        task_type='CAUSAL_LM'
        target_modules=["q_proj", "v_proj"]
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)
        base_model = AutoModel.from_pretrained(model_name, use_auth_token=auth_token, quantization_config=bnb_config).to(device)
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        base_model.resize_token_embeddings(len(tokenizer))

    base_model.gradient_checkpointing_enable()
    print(base_model.is_gradient_checkpointing)

    lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=target_modules,
            lora_dropout=0.05,
            bias="none",
            task_type=task_type
        )
    lora_model = get_peft_model(base_model, lora_config)


    # File directory
    base_data_dir = Path(__file__).parent.resolve() / 'data'

    # Training and validation data
    train_path = base_data_dir / 'train.csv'
    train_df = pd.read_csv(train_path)
    train_func = train_df['processed_func']
    train_target = train_df['target']
    train_dataset = VulnerabilityDataset(processed_func=train_func, target=train_target, tokenizer=tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    val_path = base_data_dir / 'val.csv'
    val_df = pd.read_csv(val_path)
    val_func = val_df['processed_func']
    val_target = val_df['target']
    val_dataset = VulnerabilityDataset(processed_func=val_func, target=val_target, tokenizer=tokenizer)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    # Define items for training
    model = VulnerabilityPrediction(model_name=base_model_name, model=lora_model, embedding_dim=embedding_dim)
    criterion = nn.CrossEntropyLoss()
    epochs = 5
    total_steps = len(train_loader) * epochs
    warmup_steps = int(total_steps * 0.1)
    optimizer = optim.AdamW(model.parameters(), lr=2e-5)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    model.to(device)
    print(next(model.parameters()).device)

    # Training loop and metrics
    best_f1 = 0.0
    epochs_no_improve = 0
    patience = 2
    filename = f'{base_model_name}.csv'


    all_preds, all_labels = [], [] 
    for epoch in range(epochs):
        model.train()
        tp, fp, fn, tn = 0, 0, 0, 0
        for batch_idx, (tokenized_func, label) in enumerate(train_loader):
            input_ids = tokenized_func['input_ids'].squeeze().to(device)
            attention_mask = tokenized_func['attention_mask'].squeeze().to(device)
            labels = label.to(device)
            optimizer.zero_grad()
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            scheduler.step()
            print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}")
        # Validation
        model.eval()
        with torch.no_grad():
            for batch_idx, (tokenized_func, label) in enumerate(val_loader):
                input_ids = tokenized_func['input_ids'].squeeze().to(device)
                attention_mask = tokenized_func['attention_mask'].squeeze().to(device)
                labels = label.to(device)
                logits = model(input_ids=input_ids, attention_mask=attention_mask)

                preds = (torch.argmax(logits, dim=1)).long()
                true_labels = labels.long()
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(true_labels.cpu().numpy())

                print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(val_loader)}], Loss: {loss.item()}")

            acc = accuracy_score(all_labels, all_preds)
            precision = precision_score(all_labels, all_preds)
            recall = recall_score(all_labels, all_preds)
            f1 = f1_score(all_labels, all_preds)
            mcc = matthews_corrcoef(all_labels, all_preds)
            print(f'Acc: {acc}')
            print(f'Precision: {precision}')
            print(f'Recall: {recall}')
            print(f'F1: {f1}')
            print(f'MCC: {mcc}')
            if f1 > best_f1:
                best_f1 = f1
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }, model_dir)
            else:
                epochs_no_improve += 1
            if epochs_no_improve >= patience:
                break

    # Testing
    test_path = base_data_dir / 'test.csv'
    test_df = pd.read_csv(test_path)
    test_func = test_df['processed_func']
    test_target = test_df['target']
    test_dataset = VulnerabilityDataset(processed_func=test_func, target=test_target, tokenizer=tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    model.eval()
    test_tp, test_fp, test_fn, test_tn = 0,0,0,0
    test_loss= 0.0
    test_preds, test_labels = [],[]
    with torch.no_grad():
        for batch_idx, (tokenized_func, label) in enumerate(test_loader):
            input_ids = tokenized_func['input_ids'].squeeze().to(device)
            attention_mask = tokenized_func['attention_mask'].squeeze().to(device)
            labels = label.to(device)
            logits = model(input_ids=input_ids, attention_mask=attention_mask)

            loss = criterion(logits, labels)
            test_loss += loss.item()

            preds = (torch.argmax(logits, dim=1)).long()
            true_labels = labels.long()
            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(true_labels.cpu().numpy())
        
            test_tp += ((preds == 1) & (true_labels == 1)).sum().item()
            test_fp += ((preds == 1) & (true_labels == 0)).sum().item()
            test_fn += ((preds == 0) & (true_labels == 1)).sum().item()
            test_tn += ((preds == 0) & (true_labels == 0)).sum().item()

            print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(test_loader)}], Loss: {loss.item()}")

        avg_test_loss = test_loss / len(test_loader)
        acc = accuracy_score(test_labels, test_preds)
        precision = precision_score(test_labels, test_preds)
        recall = recall_score(test_labels, test_preds)
        f1 = f1_score(test_labels, test_preds)
        mcc = matthews_corrcoef(test_labels, test_preds)
        print(f'Acc: {acc}')
        print(f'Precision: {precision}')
        print(f'Recall: {recall}')
        print(f'F1: {f1}')
        print(f'MCC: {mcc}')
        log_metrics(model_name=base_model_name, acc=acc, prec=precision, recall=recall, mcc=mcc, f1=f1, tp=test_tp, fp=test_fp, tn=test_tn, fn=test_fn, test_loss=avg_test_loss, filename=filename)
