import argparse
import csv
from pathlib import Path
import pickle
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoTokenizer

# The model, named after a rock song I enjoy and the amount of concepts that were studied and put together within its lines
class TheMotherload(nn.Module):
    def __init__(self, embedding_dim, output_channels, kernel_sizes, dropout, model_name):
        super(TheMotherload, self).__init__()
        self.name = model_name
        # Fully connected layer to process the commit message
        self.fc1 = nn.Linear(embedding_dim , output_channels * 3)
        # Fully connected layer to process the concatenation
        self.fc2 = nn.Linear(2 * output_channels * 3, 512)
        # Fully connected layer to classify
        self.fc3 = nn.Linear(512, 1)

        self.dense= nn.Linear(embedding_dim, embedding_dim)

        # We perform a 2 dimensional convolution using each k
        self.convs = nn.ModuleList([
            nn.Conv2d(in_channels=1,
                      out_channels=output_channels, 
                      kernel_size=(k, embedding_dim)) for k in kernel_sizes
        ])
        # Functions we will be using in the model
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.max_pool = nn.AdaptiveMaxPool1d(1)

    def forward(self, code, msg): 
        # Process the commmit message
        if self.name == 'codebert':
            msg = msg[:, 0, :]
        elif self.name == 'llama':
            msg = msg[:, -1, :]
        x_msg = self.fc1(msg)

        # We add a 4th dimension to match the expected input of the 2d-CNN
        x = code.unsqueeze(1)

        # x = self.dense(x)

        # This will apply the ReLU function for each extracted feature
        conv_outputs = [self.relu(conv(x)) for conv in self.convs]

        # The shape should be [batch, output_channel, sequence_length, 1], we remove the 1 here
        conv_outputs = [conv_out.squeeze(3) for conv_out in conv_outputs]

        # This will apply the max_pool function to each tensor generated by convolution (3 tensors given our default k)
        # Which will then return a tensor shape of [batch, 64, 1], we use squeeze to turn the shape to [batch, 64] 
        pooled_outputs = [self.max_pool(conv_out).squeeze(2) for conv_out in conv_outputs]
        
        # Combine the k = 1, k = 2, and k = 3 [batch, 64] sized tensors into a single [1, 192] tensor
        x = torch.cat(pooled_outputs, dim=1)
        x_code = x
        
        x_commit = torch.cat((x_msg, x_code), 1)
        x_commit = self.dropout(x_commit)
        out = self.fc2(x_commit)
        out = self.relu(out)
        out = self.fc3(out)
        return out

# We define a custom dataset for use with our CNN results and labels
class CommitDataset(Dataset):
    def __init__(self, messages, code, labels, tokenizer, embedder, device, model_name):
        self.messages = messages
        self.code = code
        self.labels = labels
        self.tokenizer = tokenizer
        self.embedder = embedder
        self.device = device
        self.model_name = model_name
    
    #The following methods are mandatory for use with DataLoader
    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # print(idx)
        # start_time = time.time()
        length = len(self.code[idx])
        code_snippet0 = ' '.join(self.code[idx][0]['added_code']) + ' ' +' '.join(self.code[idx][0]['removed_code'])
        code_snippet1 = ' '
        code_snippet2 = ' '
        code_snippet3 = ' '
        if length >= 4:
            code_snippet3 = ' '.join(self.code[idx][3]['added_code']) + ' ' +' '.join(self.code[idx][3]['removed_code'])
        if length >= 3:
            code_snippet2 = ' '.join(self.code[idx][2]['added_code']) + ' ' +' '.join(self.code[idx][2]['removed_code'])
        if length >= 2:
            code_snippet1 = ' '.join(self.code[idx][1]['added_code']) + ' ' +' '.join(self.code[idx][1]['removed_code'])

        msg_snippet = self.messages[idx]

        tokenized_code0 = self.tokenizer(code_snippet0, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(self.device)
        tokenized_code1 = self.tokenizer(code_snippet1, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(self.device)
        tokenized_code2 = self.tokenizer(code_snippet2, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(self.device)
        tokenized_code3 = self.tokenizer(code_snippet3, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(self.device)
        tokenized_msg = self.tokenizer(msg_snippet, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(self.device)

        with torch.no_grad():
            embedded_code0 = self.embedder(**tokenized_code0).last_hidden_state.squeeze(0)
            embedded_code1 = self.embedder(**tokenized_code1).last_hidden_state.squeeze(0)
            embedded_code2 = self.embedder(**tokenized_code2).last_hidden_state.squeeze(0)
            embedded_code3 = self.embedder(**tokenized_code3).last_hidden_state.squeeze(0)
            embedded_msg = self.embedder(**tokenized_msg).last_hidden_state.squeeze(0)
        embedded_code = torch.cat([embedded_code0, embedded_code1, embedded_code2, embedded_code3], dim=0)
        label = torch.tensor(self.labels[idx])
        # end_time = time.time()
        # embedtime = end_time - start_time
        # print(f'{embedtime:.4f}s')
        # if self.model_name == 'meta-llama/Llama-3.2-3B':
        #    embedded_code = embedded_code.float()
        #    embedded_msg = embedded_msg.float()
        return embedded_code, embedded_msg, label

def log_metrics(model_name, acc, prec, recall, mcc, f1, tp, fp, tn, fn, test_loss, filename):
    row = [model_name, acc, prec, recall, mcc, f1, tp, fp, tn, fn, test_loss]
    try:
        with open(filename, mode='x', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['Model','Accuracy','Precision','Recall','MCC','F1','TP','FP','TN','FN', 'Test_loss'])
    except FileExistsError:
        pass
    with open(filename, mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(row)

if __name__ == '__main__':
    # Select a model and mode to run in
    parser = argparse.ArgumentParser()
    parser.add_argument('model', type=str, choices=['codebert','llama'], help='Model selected: codebert or codellama')
    parser.add_argument('dataset', type=str, choices=['openstack','qt'], help='Dataset to use: openstack or qt')
    parser.add_argument('iterations', type=int, help='Amount of models that will be trained')
    parser.add_argument('weights', type=float, help='The pos_weight coefficient that each model will be trained with')
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Logic associated with each model
    if args.model == 'codebert':
        model_name = 'microsoft/codebert-base'
        embedding_dim = 768
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        embedder = AutoModel.from_pretrained(model_name).to(device)
    elif args.model == 'llama':
        model_name = 'meta-llama/Llama-3.2-1B'
        embedding_dim = 2048
        # This is a gated model, please save your Hugging Face read-only token in token.txt
        with open('token.txt', 'r') as file:
            auth_token = file.read().strip()
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)
        embedder = AutoModel.from_pretrained(model_name, use_auth_token=auth_token).to(device)
        # embedder = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16, use_auth_token=auth_token).to(device)
        # We need to add special padding tokens to decoder models and update the length of its volcabulary to include the new pad token
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        embedder.resize_token_embeddings(len(tokenizer))

    # Define our model variables
    dropout = 0.5
    kernel_sizes = [1, 2 ,3]
    output_channels = 64
    pos_weight_coefficient = args.weights
    # Using this for a filename later, cannot have a decimal place in it
    pos_name = int(args.weights*100)

    # File directory
    base_data_dir = Path(__file__).parent.resolve() / 'data/data_and_model/data+model/data/jit'
    if args.dataset == 'qt':
        train_path = base_data_dir / 'qt_train.pkl'
        test_path = base_data_dir / 'qt_test.pkl'
    else:
        train_path = base_data_dir / 'openstack_train.pkl'
        test_path = base_data_dir / 'openstack_test.pkl'
    with open(train_path, 'rb') as f:
        train_data = pickle.load(f)
    with open(test_path, 'rb') as f2:
        test_data = pickle.load(f2)

    # Unpack the tuple
    train_commit_id, train_label, train_msg, train_code = train_data
    test_commit_id, test_label, test_msg, test_code = test_data

    # Begin setting up necessary items for training
    train_dataset = CommitDataset(train_msg, train_code, train_label, tokenizer, embedder, device, model_name)
    test_dataset = CommitDataset(test_msg, test_code, test_label, tokenizer, embedder, device, model_name)
    trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Figure out how much to penalize false negatives
    # The idea is that you count how many positive and negative
    # samples you have in your dataset, and then figure out
    # some sort of weight that will be larger the bigger the disparity
    # Here we go with a simple ratio of negatives to positive 
    # multiplied by an adjustable coefficient for finetuning, 
    # but could potentially do more sophisticated things
    y = torch.tensor(train_dataset.labels)
    n_pos = (y == 1).sum().item()
    n_neg = (y == 0).sum().item()
    # Weight so that positives contribute as much as negatives
    pos_weight = torch.tensor([ pos_weight_coefficient * n_neg / n_pos], dtype=torch.float32, device=device)
    print(f"Using coefficient {pos_weight_coefficient} which adds a weight of {pos_weight} to positive results to increase loss for false negatives")

    num_epochs = 5

    # Loop to train, test, and save multiple models and results
    for i in range(args.iterations):
        model = TheMotherload(embedding_dim=embedding_dim, output_channels=output_channels, kernel_sizes=kernel_sizes, dropout=dropout, model_name=args.model)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        optimizer = optim.Adam(model.parameters(), lr=1e-5)
        model.to(device)
        print(next(model.parameters()).device)
        filename = f'{args.model}{args.dataset}{pos_name}.csv'
        model_name = f'{args.model}{args.dataset}{pos_name}{(i + 1)}'
        save_dir = f'{model_name}.pth'

        # Training
        for epoch in range(num_epochs):
            model.train()
            running_loss, tp, fp, fn, tn = 0, 0, 0, 0, 0
            for batch_idx, (code_embeddings, msg_embeddings, labels_tensor) in enumerate(trainloader):
                code_embeddings, msg_embeddings, labels = code_embeddings.to(device), msg_embeddings.to(device), labels_tensor.to(device)
                optimizer.zero_grad()
                outputs = model(code_embeddings, msg_embeddings)
                loss = criterion(outputs.squeeze(), labels.float())
                loss.backward()
                optimizer.step()
                running_loss += loss.item()

                preds = (torch.sigmoid(outputs).squeeze() > 0.5).long()
                labels = labels.long()
                tp += ((preds == 1) & (labels == 1)).sum().item()
                fp += ((preds == 1) & (labels == 0)).sum().item()
                fn += ((preds == 0) & (labels == 1)).sum().item()
                tn += ((preds == 0) & (labels == 0)).sum().item()

                print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(trainloader)}], Loss: {loss.item()}")
            avg_loss = running_loss / len(trainloader)
            print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}")
            print(f"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
        
        # Testing
        model.eval()
        test_loss= 0.0
        test_tp, test_fp, test_tn, test_fn = 0, 0, 0, 0
        all_preds, all_labels, all_logits = [], [], []
        with torch.no_grad():
            for batch_idx, (code_embeddings, msg_embeddings, labels_tensor) in enumerate(testloader):
                code_embeddings, msg_embeddings, labels = code_embeddings.to(device), msg_embeddings.to(device), labels_tensor.to(device)
                outputs = model(code_embeddings, msg_embeddings)
                
                tloss = criterion(outputs.squeeze(), labels.float())
                test_loss += tloss.item()
                all_logits.extend(outputs.squeeze().cpu().numpy())
                
                preds = (torch.sigmoid(outputs).squeeze() > 0.5).long()
                true_labels = labels.long()
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(true_labels.cpu().numpy())

                test_tp += ((preds == 1) & (labels == 1)).sum().item()
                test_fp += ((preds == 1) & (labels == 0)).sum().item()
                test_fn += ((preds == 0) & (labels == 1)).sum().item()
                test_tn += ((preds == 0) & (labels == 0)).sum().item()
                print(f"Batch [{batch_idx}/{len(testloader)}], Loss: {tloss.item()}")

            avg_test_loss = test_loss / len(testloader)
            acc = accuracy_score(all_labels, all_preds)
            precision = precision_score(all_labels, all_preds)
            recall = recall_score(all_labels, all_preds)
            f1 = f1_score(all_labels, all_preds)
            mcc = matthews_corrcoef(all_labels, all_preds)

            print(f"Test Loss: {avg_test_loss:.4f}")
            print(f"Test Accuracy: {acc:.4f}")
            print(f"Test F1 Score: {f1:.4f}")
            print(f"Test MCC: {mcc:.4f}")
                
            print(f"Test Precision: {precision:.4f}")
            print(f"Test Recall: {recall:.4f}")
            log_metrics(model_name=model_name, acc=acc, prec=precision, recall=recall, mcc=mcc, f1=f1, tp=test_tp, fp=test_fp, tn=test_tn, fn=test_fn, test_loss=avg_test_loss, filename=filename)

            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'avg_test_loss': avg_test_loss
            }, save_dir)
